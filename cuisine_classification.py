# -*- coding: utf-8 -*-
"""Cuisine Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iK6eibpyrLsR9YcjXg5BWuHE0JacRtSS

# Step 1: Install Required Libraries
"""

# Install any additional packages if needed
!pip install pandas scikit-learn matplotlib seaborn

"""# Step 2:Import Required Libraries"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

"""# Step 3: Load and Explore the Dataset"""

# Load the dataset (adjust the path to where you've uploaded the file)
file_path = '/content/Dataset .csv'  # Update this path
df = pd.read_csv(file_path)

# Display basic info about the dataset
print(df.info())
print(df.head())

# Check for missing values
print(df.isnull().sum())

"""# Step 4: Data Preprocessing"""

# Handle missing values in the 'Cuisines' column
df['Cuisines'] = df['Cuisines'].fillna('Unknown')

# For this task, we'll focus on cuisine classification using restaurant name and cuisine description
# Let's extract the primary cuisine (first one listed)
df['Primary_Cuisine'] = df['Cuisines'].apply(lambda x: x.split(',')[0].strip())

# Count the frequency of each primary cuisine
cuisine_counts = df['Primary_Cuisine'].value_counts()
print(cuisine_counts)

# Let's focus on the top N cuisines to simplify the classification
top_n = 10  # You can adjust this number
top_cuisines = cuisine_counts.index[:top_n]
df_filtered = df[df['Primary_Cuisine'].isin(top_cuisines)]

# Encode the target variable (cuisine)
label_encoder = LabelEncoder()
df_filtered['Cuisine_Label'] = label_encoder.fit_transform(df_filtered['Primary_Cuisine'])

# Prepare features - we'll use restaurant name and other text features
features = df_filtered[['Restaurant Name', 'Cuisines', 'City', 'Locality Verbose']]
target = df_filtered['Cuisine_Label']

# Combine text features into a single column
features['Combined_Text'] = features['Restaurant Name'] + ' ' + features['Cuisines'] + ' ' + features['City'] + ' ' + features['Locality Verbose']

"""# Step 5: Split Data into Training and Testing Sets"""

X_train, X_test, y_train, y_test = train_test_split(
    features['Combined_Text'],
    target,
    test_size=0.2,
    random_state=42,
    stratify=target
)

print(f"Training set size: {len(X_train)}")
print(f"Testing set size: {len(X_test)}")

"""# Step 6: Feature Engineering with TF-IDF"""

# Create TF-IDF vectorizer
tfidf = TfidfVectorizer(
    max_features=5000,  # Limit number of features to prevent overfitting
    stop_words='english',
    ngram_range=(1, 2)  # Consider both single words and word pairs
)

# Fit and transform the training data
X_train_tfidf = tfidf.fit_transform(X_train)

# Transform the test data
X_test_tfidf = tfidf.transform(X_test)

"""# Step 7: Train Classification Models

## Option 1: Logistic Regression
"""

# Logistic Regression model
logreg = LogisticRegression(
    max_iter=1000,
    multi_class='multinomial',
    solver='lbfgs',
    random_state=42
)

logreg.fit(X_train_tfidf, y_train)

# Predictions
y_pred_logreg = logreg.predict(X_test_tfidf)

# Evaluation
print("Logistic Regression Performance:")
print(classification_report(y_test, y_pred_logreg, target_names=label_encoder.classes_))
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))

# Confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix(y_test, y_pred_logreg),
            annot=True, fmt='d',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix - Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""## Option 2: Random Forest"""

# Random Forest model
rf = RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    class_weight='balanced'  # Helps with imbalanced classes
)

rf.fit(X_train_tfidf, y_train)

# Predictions
y_pred_rf = rf.predict(X_test_tfidf)

# Evaluation
print("Random Forest Performance:")
print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))
print("Accuracy:", accuracy_score(y_test, y_pred_rf))

# Confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(confusion_matrix(y_test, y_pred_rf),
            annot=True, fmt='d',
            xticklabels=label_encoder.classes_,
            yticklabels=label_encoder.classes_)
plt.title('Confusion Matrix - Random Forest')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""# Step 8: Model Analysis and Interpretation"""

# Feature importance for Logistic Regression
feature_names = tfidf.get_feature_names_out()
coefs = logreg.coef_

# Plot top features for each cuisine
plt.figure(figsize=(15, 10))
for i, cuisine in enumerate(label_encoder.classes_):
    top_features = np.argsort(coefs[i])[-10:]  # Top 10 features
    plt.subplot(3, 4, i+1)
    plt.barh(range(10), coefs[i][top_features])
    plt.yticks(range(10), [feature_names[j] for j in top_features])
    plt.title(f"Top features for {cuisine}")
plt.tight_layout()
plt.show()

# Compare model performances
print("Model Comparison:")
print(f"Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_logreg)}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, y_pred_rf)}")

"""# Step 9: Handling Class Imbalance (Optional)"""

# Option 1: Adjust class weights in the models
# Already done in Random Forest with class_weight='balanced'
# For Logistic Regression:
logreg_balanced = LogisticRegression(
    max_iter=1000,
    multi_class='multinomial',
    solver='lbfgs',
    random_state=42,
    class_weight='balanced'
)
logreg_balanced.fit(X_train_tfidf, y_train)
y_pred_logreg_balanced = logreg_balanced.predict(X_test_tfidf)
print("Balanced Logistic Regression Performance:")
print(classification_report(y_test, y_pred_logreg_balanced, target_names=label_encoder.classes_))

# Option 2: Use SMOTE for oversampling minority classes
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)

# Train a model on the balanced data
logreg_smote = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs', random_state=42)
logreg_smote.fit(X_train_smote, y_train_smote)
y_pred_logreg_smote = logreg_smote.predict(X_test_tfidf)
print("SMOTE + Logistic Regression Performance:")
print(classification_report(y_test, y_pred_logreg_smote, target_names=label_encoder.classes_))

"""## Example of how to use the trained model to classify a new restaurant

### This will:
Take a new restaurant description as input
Transform it using the same TF-IDF vectorizer we trained
Predict the cuisine using the logistic regression model
Convert the numerical label back to the actual cuisine name
Print the predicted cuisine


"""

# Example of how to use the trained model to classify a new restaurant
new_restaurant_data = ["Sushi Palace Japanese Restaurant in Tokyo serving fresh sushi and sashimi"]
new_data_tfidf = tfidf.transform(new_restaurant_data)
predicted_label = logreg.predict(new_data_tfidf)
predicted_cuisine = label_encoder.inverse_transform(predicted_label)
print(f"Predicted cuisine: {predicted_cuisine[0]}")

"""# Developed By Manikandan M
## Thank You.
"""



